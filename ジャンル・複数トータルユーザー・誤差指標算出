# ==============================================================================
# Step 0: å¿…è¦ãªãƒ©ã‚¤ãƒ–ãƒ©ãƒªã®ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ã¨èªè¨¼
# ==============================================================================
!pip install --upgrade google-api-python-client google-auth-httplib2 google-auth-oauthlib gspread gspread-dataframe japanize-matplotlib scikit-learn

import pandas as pd
import numpy as np
from scipy.optimize import minimize
from scipy.special import gammaln
import matplotlib.pyplot as plt
import japanize_matplotlib
import gspread
from google.colab import auth
from googleapiclient.discovery import build
from googleapiclient.http import MediaFileUpload
from google.auth import default
from datetime import datetime
import pytz
from gspread_dataframe import set_with_dataframe
from sklearn.metrics import mean_absolute_error, mean_squared_error
import os

# Google Colabã§ã®èªè¨¼
auth.authenticate_user()
creds, _ = default()
gc = gspread.authorize(creds)
drive_service = build('drive', 'v3', credentials=creds)

print("âœ… èªè¨¼ã¨ãƒ©ã‚¤ãƒ–ãƒ©ãƒªã®æº–å‚™ãŒå®Œäº†ã—ã¾ã—ãŸã€‚")
print("-" * 50)


# ==============================================================================
# â–¼â–¼â–¼â–¼â–¼â–¼â–¼â–¼â–¼â–¼â–¼â–¼â–¼â–¼â–¼â–¼â–¼â–¼â–¼â–¼â–¼â–¼ è¨­å®šãƒ–ãƒ­ãƒƒã‚¯ â–¼â–¼â–¼â–¼â–¼â–¼â–¼â–¼â–¼â–¼â–¼â–¼â–¼â–¼â–¼â–¼â–¼â–¼â–¼â–¼â–¼â–¼â–¼â–¼â–¼â–¼
# ==============================================================================
TARGET_FOLDER_NAME = '20251019_VODãƒ‡ãƒ¼ã‚¿åˆ†æ'
WORKSHEET_NAME_TO_READ = 'é›†è¨ˆã‚¹ãƒãƒ¼ãƒ„'
TOTAL_VOD_USERS_LIST = [110000, 120000, 130000, 140000,150000, 160000, 170000,180000,190000,200000]
IMAGE_OUTPUT_FOLDER_NAME = 'NBDåˆ†æã‚°ãƒ©ãƒ•å‡ºåŠ›'

jst = pytz.timezone('Asia/Tokyo')
current_time_str = datetime.now(jst).strftime('%Y%m%d_%H%M%S')
OUTPUT_SPREADSHEET_NAME = f'NBDãƒ¢ãƒ‡ãƒ«åˆ†æçµæœ_{current_time_str}'
# ==============================================================================
# â–²â–²â–²â–²â–²â–²â–²â–²â–²â–²â–²â–²â–²â–²â–²â–²â–²â–²â–²â–²â–²â–² è¨­å®šã¯ã“ã“ã¾ã§ â–²â–²â–²â–²â–²â–²â–²â–²â–²â–²â–²â–²â–²â–²â–²â–²â–²â–²â–²â–²â–²â–²â–²â–²â–²
# ==============================================================================


# ==============================================================================
# Step 1 & 2: Google Driveã‹ã‚‰ãƒ‡ãƒ¼ã‚¿ã‚’å–å¾—ã—ã€çµåˆã™ã‚‹
# ==============================================================================
# â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…
# ã€ä»Šå›ã®ä¿®æ­£ç‚¹ã€‘
# ã‚¨ãƒ©ãƒ¼ã®åŸå› ã¨ãªã£ã¦ã„ãŸãƒ†ã‚¹ãƒˆç”¨ã®CSVèª­ã¿è¾¼ã¿ã‚³ãƒ¼ãƒ‰ã‚’å®Œå…¨ã«å‰Šé™¤ã—ã¾ã—ãŸã€‚
# â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…
print(f"\nStep 1 & 2: Google Drive '{TARGET_FOLDER_NAME}' ã‹ã‚‰ãƒ‡ãƒ¼ã‚¿ã‚’èª­ã¿è¾¼ã¿ã€çµåˆã—ã¾ã™...")
all_dataframes = []
spreadsheet_files = []
try:
    folder_response = drive_service.files().list(
        q=f"name='{TARGET_FOLDER_NAME}' and mimeType='application/vnd.google-apps.folder' and trashed=false",
        spaces='drive', fields='files(id, name)').execute()
    folders = folder_response.get('files', [])
    if not folders:
        print(f"-> âŒã€ã‚¨ãƒ©ãƒ¼ï¼ã€‘ãƒ•ã‚©ãƒ«ãƒ€ '{TARGET_FOLDER_NAME}' ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã€‚")
    else:
        folder_id = folders[0].get('id')
        page_token = None
        sheets_in_folder = []
        while True:
            sheet_response = drive_service.files().list(
                q=f"'{folder_id}' in parents and mimeType='application/vnd.google-apps.spreadsheet' and trashed=false",
                spaces='drive', fields='nextPageToken, files(id, name)', pageToken=page_token).execute()
            sheets_in_folder.extend(sheet_response.get('files', []))
            page_token = sheet_response.get('nextPageToken', None)
            if page_token is None: break
        if sheets_in_folder:
            spreadsheet_files = sheets_in_folder
except Exception as e:
    print(f"-> âŒã€ã‚¨ãƒ©ãƒ¼ï¼ã€‘ãƒ•ã‚©ãƒ«ãƒ€æ¤œç´¢ä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {e}")

if spreadsheet_files:
    print(f"-> âœ… {len(spreadsheet_files)}å€‹ã®ã‚¹ãƒ—ãƒ¬ãƒƒãƒ‰ã‚·ãƒ¼ãƒˆãŒè¦‹ã¤ã‹ã‚Šã¾ã—ãŸã€‚")
    for sheet_file in spreadsheet_files:
        file_name = sheet_file.get('name')
        print(f"ğŸ“„ '{file_name}' ã® '{WORKSHEET_NAME_TO_READ}' ã‚·ãƒ¼ãƒˆã‚’èª­ã¿è¾¼ã‚“ã§ã„ã¾ã™..." )
        try:
            spreadsheet = gc.open(file_name)
            worksheet = spreadsheet.worksheet(WORKSHEET_NAME_TO_READ)
            data = worksheet.get_all_records()
            df = pd.DataFrame.from_records(data)
            df = df.loc[:, ~df.columns.str.contains('^Unnamed')]
            all_dataframes.append(df)
            print(f"-> âœ… èª­ã¿è¾¼ã¿æˆåŠŸï¼ {len(df)} ä»¶ã®ãƒ‡ãƒ¼ã‚¿ã‚’å–å¾—ã—ã¾ã—ãŸã€‚")
        except gspread.exceptions.WorksheetNotFound:
            print(f"-> âŒã€ã‚¨ãƒ©ãƒ¼ï¼ã€‘ã‚·ãƒ¼ãƒˆ '{WORKSHEET_NAME_TO_READ}' ãŒ '{file_name}' ã«è¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã€‚")
        except Exception as e:
            print(f"-> âŒã€ã‚¨ãƒ©ãƒ¼ï¼ã€‘: {e}")
else:
    print(f"-> âŒã€ã‚¨ãƒ©ãƒ¼ï¼ã€‘ãƒ•ã‚©ãƒ«ãƒ€å†…ã«ã‚¹ãƒ—ãƒ¬ãƒƒãƒ‰ã‚·ãƒ¼ãƒˆãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã€‚")

if all_dataframes:
    df_main = pd.concat(all_dataframes, ignore_index=True)
    print(f"âœ… å…¨ãƒ‡ãƒ¼ã‚¿ã®çµåˆãŒå®Œäº†ã—ã¾ã—ãŸã€‚åˆè¨ˆ {len(df_main)} ä»¶ã®ãƒ‡ãƒ¼ã‚¿ã€‚")
else:
    df_main = pd.DataFrame()
    print("âš ï¸ èª­ã¿è¾¼ã‚ã‚‹ãƒ‡ãƒ¼ã‚¿ãŒãªã‹ã£ãŸãŸã‚ã€åˆ†æã¯å®Ÿè¡Œã•ã‚Œã¾ã›ã‚“ã€‚")
print("-" * 50)


# ==============================================================================
# Step 3: åˆ†æã¨å¯è¦–åŒ–ã‚’è¡Œã†é–¢æ•°ã‚’å®šç¾©
# ==============================================================================
def analyze_nbd_fit(data_df, title, total_users):
    print(f"--- åˆ†æé–‹å§‹: {title} (ç·ãƒ¦ãƒ¼ã‚¶ãƒ¼æ•°: {total_users}) ---")
    if data_df.empty or data_df['é »åº¦ã”ã¨ã®ãƒ¦ãƒ‹ãƒ¼ã‚¯ãƒ¦ãƒ¼ã‚¶ãƒ¼'].sum() == 0:
        return None, None, None, None
    data_df['è¦–è´é »åº¦_æ•°å€¤'] = pd.to_numeric(data_df['è¦–è´é »åº¦'].astype(str).str.replace('+', ''), errors='coerce')
    program_unique_users = data_df['ç•ªçµ„ã®ãƒ¦ãƒ‹ãƒ¼ã‚¯ãƒ¦ãƒ¼ã‚¶ãƒ¼'].iloc[0]
    program_total_views = data_df['ç•ªçµ„ã®ç·è¦–è´å›æ•°'].iloc[0]
    if total_users == 0 or program_unique_users == 0: return None, None, None, None
    b = program_unique_users / total_users; w = program_total_views / program_unique_users; M = w * b
    def nbd_probability(r, K, M):
        with np.errstate(divide='ignore', invalid='ignore'):
            log_p = gammaln(K + r) - gammaln(K) - gammaln(r + 1) + r * np.log(M / (M + K)) + K * np.log(K / (K + M))
        return np.exp(log_p)
    def objective_function(K, data, M):
        K = K[0]
        if K <= 0: return np.inf
        max_freq = data['è¦–è´é »åº¦_æ•°å€¤'].max(); has_plus_category = '+' in str(data['è¦–è´é »åº¦'].iloc[-1])
        if has_plus_category:
            r_values_prob = np.arange(0, max_freq)
            probabilities = nbd_probability(r_values_prob, K, M)
            prob_plus = 1 - probabilities.sum(); full_probabilities = np.append(probabilities, prob_plus)
            actuals_freq = pd.Series(data['é »åº¦ã”ã¨ã®ãƒ¦ãƒ‹ãƒ¼ã‚¯ãƒ¦ãƒ¼ã‚¶ãƒ¼'].values, index=data['è¦–è´é »åº¦_æ•°å€¤'].values)
            r_values_actual = np.arange(1, max_freq + 1); actuals_reindexed = actuals_freq.reindex(r_values_actual, fill_value=0).values
            actuals = np.concatenate(([total_users - program_unique_users], actuals_reindexed))
        else:
            r_values = np.arange(0, max_freq + 1); full_probabilities = nbd_probability(r_values, K, M)
            actuals_freq = pd.Series(data['é »åº¦ã”ã¨ã®ãƒ¦ãƒ‹ãƒ¼ã‚¯ãƒ¦ãƒ¼ã‚¶ãƒ¼'].values, index=data['è¦–è´é »åº¦_æ•°å€¤'].values)
            actuals = actuals_freq.reindex(r_values, fill_value=0).values; actuals[0] = total_users - program_unique_users
        full_probabilities[full_probabilities <= 0] = 1e-10
        if len(actuals) != len(full_probabilities): return np.inf
        log_likelihood = np.sum(actuals * np.log(full_probabilities)); return -log_likelihood
    result = minimize(objective_function, [1.0], args=(data_df, M), bounds=[(1e-6, None)])
    optimal_K = result.x[0]
    if optimal_K > 1e5 or not result.success: return None, None, None, None
    print(f"-> æ¨å®šã•ã‚ŒãŸæœ€é©ãªKãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿: {optimal_K:.4f}")
    prob_0 = nbd_probability(0, optimal_K, M)
    r_plot = data_df['è¦–è´é »åº¦_æ•°å€¤']; theoretical_probs = nbd_probability(r_plot, optimal_K, M)
    if '+' in str(data_df['è¦–è´é »åº¦'].iloc[-1]):
      prob_sum_until_9 = nbd_probability(np.arange(0, 10), optimal_K, M).sum()
      theoretical_probs.iloc[-1] = (1 - prob_sum_until_9)
    theoretical_users = (theoretical_probs / (1 - prob_0)) * program_unique_users
    result_df = pd.DataFrame({'è¦–è´é »åº¦': data_df['è¦–è´é »åº¦'].astype(str), 'å®Ÿç¸¾ãƒ¦ãƒ¼ã‚¶ãƒ¼æ•°': data_df['é »åº¦ã”ã¨ã®ãƒ¦ãƒ‹ãƒ¼ã‚¯ãƒ¦ãƒ¼ã‚¶ãƒ¼'], 'ç†è«–ãƒ¦ãƒ¼ã‚¶ãƒ¼æ•°ï¼ˆNBDãƒ¢ãƒ‡ãƒ«ï¼‰': theoretical_users.round(2)})
    actual = result_df['å®Ÿç¸¾ãƒ¦ãƒ¼ã‚¶ãƒ¼æ•°']; predicted = result_df['ç†è«–ãƒ¦ãƒ¼ã‚¶ãƒ¼æ•°ï¼ˆNBDãƒ¢ãƒ‡ãƒ«ï¼‰']
    mae = mean_absolute_error(actual, predicted); rmse = np.sqrt(mean_squared_error(actual, predicted))
    error_metrics = {'MAE': mae, 'RMSE': rmse}
    print(f"-> MAE: {mae:.2f}, RMSE: {rmse:.2f}")
    fig = plt.figure(figsize=(12, 7))
    plt.bar(result_df['è¦–è´é »åº¦'], result_df['å®Ÿç¸¾ãƒ¦ãƒ¼ã‚¶ãƒ¼æ•°'], label='å®Ÿç¸¾ãƒ¦ãƒ¼ã‚¶ãƒ¼æ•°', color='skyblue', zorder=2)
    plt.plot(result_df['è¦–è´é »åº¦'], result_df['ç†è«–ãƒ¦ãƒ¼ã‚¶ãƒ¼æ•°ï¼ˆNBDãƒ¢ãƒ‡ãƒ«ï¼‰'], 'o-', label='NBDãƒ¢ãƒ‡ãƒ«ã«ã‚ˆã‚‹ç†è«–å€¤', color='tomato', markersize=8, zorder=3)
    plt.xlabel('è¦–è´å›æ•° (r)', fontsize=14); plt.ylabel('ãƒ¦ãƒ‹ãƒ¼ã‚¯ãƒ¦ãƒ¼ã‚¶ãƒ¼æ•°', fontsize=14); plt.title(f'ã€{title}ã€‘\nNBDãƒ¢ãƒ‡ãƒ«é©åˆåº¦åˆ†æ (ç·ãƒ¦ãƒ¼ã‚¶ãƒ¼æ•°={total_users}, æ¨å®šK={optimal_K:.4f})', fontsize=16)
    plt.legend(fontsize=12); plt.grid(axis='y', linestyle='--', alpha=0.7)
    image_filename = f"chart_{title.replace(':', '').replace('/', '').replace(' ', '_')}_U{total_users}.png"
    plt.savefig(image_filename); plt.show(); plt.close(fig)
    print("-" * 50)
    return result_df, optimal_K, error_metrics, image_filename


# ==============================================================================
# Step 4: è¤‡æ•°ã‚·ãƒŠãƒªã‚ªã§åˆ†æã‚’å®Ÿè¡Œ ï¼† çµæœã‚’ã‚¹ãƒ—ãƒ¬ãƒƒãƒ‰ã‚·ãƒ¼ãƒˆã«å‡ºåŠ›
# ==============================================================================
if not df_main.empty:
    print("\n\nStep 4: å…¨ã‚·ãƒŠãƒªã‚ªã§åˆ†æã‚’å®Ÿè¡Œã—ã€ã‚¹ãƒ—ãƒ¬ãƒƒãƒ‰ã‚·ãƒ¼ãƒˆã«å‡ºåŠ›ã—ã¾ã™...")
    output_spreadsheet = None; image_folder_id = None
    try:
        folder_response = drive_service.files().list(q=f"name='{IMAGE_OUTPUT_FOLDER_NAME}' and mimeType='application/vnd.google-apps.folder' and trashed=false", spaces='drive', fields='files(id)').execute()
        folders = folder_response.get('files', [])
        if folders:
            image_folder_id = folders[0]['id']; print(f"âœ… ã‚°ãƒ©ãƒ•ä¿å­˜ç”¨ãƒ•ã‚©ãƒ«ãƒ€ '{IMAGE_OUTPUT_FOLDER_NAME}' ãŒè¦‹ã¤ã‹ã‚Šã¾ã—ãŸã€‚")
        else:
            folder_metadata = {'name': IMAGE_OUTPUT_FOLDER_NAME, 'mimeType': 'application/vnd.google-apps.folder'}
            folder = drive_service.files().create(body=folder_metadata, fields='id').execute()
            image_folder_id = folder.get('id'); print(f"âœ… ã‚°ãƒ©ãƒ•ä¿å­˜ç”¨ãƒ•ã‚©ãƒ«ãƒ€ '{IMAGE_OUTPUT_FOLDER_NAME}' ã‚’æ–°è¦ä½œæˆã—ã¾ã—ãŸã€‚")
    except Exception as e: print(f"-> âŒã€ã‚¨ãƒ©ãƒ¼ï¼ã€‘ã‚°ãƒ©ãƒ•ä¿å­˜ç”¨ãƒ•ã‚©ãƒ«ãƒ€ã®æº–å‚™ä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {e}")
    try:
        output_spreadsheet = gc.create(OUTPUT_SPREADSHEET_NAME)
        summary_sheet = output_spreadsheet.sheet1; summary_sheet.update_title('ã‚µãƒãƒªãƒ¼')
        print(f"âœ… çµæœå‡ºåŠ›ç”¨ã®ã‚¹ãƒ—ãƒ¬ãƒƒãƒ‰ã‚·ãƒ¼ãƒˆ '{OUTPUT_SPREADSHEET_NAME}' ã‚’ä½œæˆã—ã¾ã—ãŸã€‚")
    except Exception as e: print(f"-> âŒã€ã‚¨ãƒ©ãƒ¼ï¼ã€‘ã‚¹ãƒ—ãƒ¬ãƒƒãƒ‰ã‚·ãƒ¼ãƒˆã®ä½œæˆã«å¤±æ•—ã—ã¾ã—ãŸ: {e}")

    summary_data = []

    if output_spreadsheet and image_folder_id:
        genre_groups = df_main.groupby('genre')
        for genre_name, df_genre in genre_groups:
            try:
                worksheet = output_spreadsheet.add_worksheet(title=genre_name, rows=1000, cols=30)
                current_row = 1
            except Exception as e: print(f"-> âŒã€ã‚¨ãƒ©ãƒ¼ï¼ã€‘ã‚·ãƒ¼ãƒˆ '{genre_name}' ã®ä½œæˆã«å¤±æ•—ã—ã¾ã—ãŸ: {e}"); continue

            for total_users_scenario in TOTAL_VOD_USERS_LIST:
                df_genre_agg = df_genre.groupby('è¦–è´é »åº¦')['é »åº¦ã”ã¨ã®ãƒ¦ãƒ‹ãƒ¼ã‚¯ãƒ¦ãƒ¼ã‚¶ãƒ¼'].sum().reset_index()
                genre_unique_users = df_genre.groupby('program_title')['ç•ªçµ„ã®ãƒ¦ãƒ‹ãƒ¼ã‚¯ãƒ¦ãƒ¼ã‚¶ãƒ¼'].first().sum()
                genre_total_views = df_genre.groupby('program_title')['ç•ªçµ„ã®ç·è¦–è´å›æ•°'].first().sum()
                df_genre_agg['ç•ªçµ„ã®ãƒ¦ãƒ‹ãƒ¼ã‚¯ãƒ¦ãƒ¼ã‚¶ãƒ¼'] = genre_unique_users; df_genre_agg['ç•ªçµ„ã®ç·è¦–è´å›æ•°'] = genre_total_views
                df_genre_agg['sort_key'] = pd.to_numeric(df_genre_agg['è¦–è´é »åº¦'].astype(str).str.replace('+', ''), errors='coerce')
                df_genre_agg = df_genre_agg.sort_values('sort_key').drop('sort_key', axis=1)

                result_df, optimal_K, error_metrics, image_filename = analyze_nbd_fit(df_genre_agg, f"ã‚¸ãƒ£ãƒ³ãƒ«: {genre_name}", total_users_scenario)

                if result_df is not None:
                    summary_data.append({'ã‚¸ãƒ£ãƒ³ãƒ«': genre_name, 'ç·ãƒ¦ãƒ¼ã‚¶ãƒ¼æ•°ã‚·ãƒŠãƒªã‚ª': total_users_scenario, 'æ¨å®šã•ã‚ŒãŸKãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿': optimal_K, 'MAE': error_metrics['MAE'], 'RMSE': error_metrics['RMSE']})
                    header_df = pd.DataFrame({'é …ç›®': [f'ç·ãƒ¦ãƒ¼ã‚¶ãƒ¼æ•°ã‚·ãƒŠãƒªã‚ª', 'æ¨å®šã•ã‚ŒãŸKãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿', 'MAE (å¹³å‡çµ¶å¯¾èª¤å·®)', 'RMSE (äºŒä¹—å¹³å‡å¹³æ–¹æ ¹èª¤å·®)'], 'å€¤': [f'{total_users_scenario}', f'{optimal_K:.4f}', f"{error_metrics['MAE']:.2f}", f"{error_metrics['RMSE']:.2f}"]})
                    set_with_dataframe(worksheet, header_df, row=current_row, col=1, include_index=False)
                    set_with_dataframe(worksheet, result_df, row=current_row, col=3) # Dåˆ—ã‹ã‚‰è¡¨ã‚’æ›¸ãè¾¼ã¿
                    try:
                        file_metadata = {'name': image_filename, 'parents': [image_folder_id]}
                        media = MediaFileUpload(image_filename, mimetype='image/png')
                        image_file = drive_service.files().create(body=file_metadata, media_body=media, fields='id').execute()
                        image_id = image_file.get('id')
                        drive_service.permissions().create(fileId=image_id, body={'type': 'anyone', 'role': 'reader'}).execute()
                        image_url = f'https://drive.google.com/uc?id={image_id}'
                        worksheet.update_acell(f'J{current_row}', f'=IMAGE("{image_url}")') # Jåˆ—ã«ç”»åƒã‚’æŒ¿å…¥
                        print(f"-> âœ… ã‚°ãƒ©ãƒ•ç”»åƒã‚’ã‚·ãƒ¼ãƒˆã® J{current_row} ã«æŒ¿å…¥ã—ã¾ã—ãŸã€‚")
                        os.remove(image_filename)
                    except Exception as e: print(f"-> âŒã€ã‚¨ãƒ©ãƒ¼ï¼ã€‘ã‚°ãƒ©ãƒ•ç”»åƒã®åŸ‹ã‚è¾¼ã¿ä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {e}")
                    current_row += len(result_df) + 3
            print(f"-> âœ… ã‚¸ãƒ£ãƒ³ãƒ« '{genre_name}' ã®å…¨ã‚·ãƒŠãƒªã‚ªçµæœã‚’ã‚·ãƒ¼ãƒˆã«æ›¸ãè¾¼ã¿ã¾ã—ãŸã€‚")

        if summary_data:
            try:
                summary_df_all = pd.DataFrame(summary_data)
                summary_pivot = pd.pivot_table(summary_df_all, index='ã‚¸ãƒ£ãƒ³ãƒ«', columns='ç·ãƒ¦ãƒ¼ã‚¶ãƒ¼æ•°ã‚·ãƒŠãƒªã‚ª', values=['æ¨å®šã•ã‚ŒãŸKãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿', 'MAE', 'RMSE'])
                summary_pivot = summary_pivot.sort_values(by=('æ¨å®šã•ã‚ŒãŸKãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿', TOTAL_VOD_USERS_LIST[0]))
                set_with_dataframe(summary_sheet, summary_pivot.reset_index())
                print("\n-> âœ… å…¨ã‚·ãƒŠãƒªã‚ªã®Kãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã¨èª¤å·®æŒ‡æ¨™ã‚’æ¯”è¼ƒã—ãŸã€Œã‚µãƒãƒªãƒ¼ã€ã‚·ãƒ¼ãƒˆã‚’æ›´æ–°ã—ã¾ã—ãŸã€‚")
            except Exception as e: print(f"-> âŒã€ã‚¨ãƒ©ãƒ¼ï¼ã€‘ã‚µãƒãƒªãƒ¼ã‚·ãƒ¼ãƒˆã®ä½œæˆä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {e}")

        print("\n\nğŸ‰ å…¨ã¦ã®å‡¦ç†ãŒå®Œäº†ã—ã¾ã—ãŸï¼ ğŸ‰")
        print(f"åˆ†æçµæœã¯ä»¥ä¸‹ã®ã‚¹ãƒ—ãƒ¬ãƒƒãƒ‰ã‚·ãƒ¼ãƒˆã«å‡ºåŠ›ã•ã‚Œã¦ã„ã¾ã™:")
        print(f"ğŸ”— {output_spreadsheet.url}")
